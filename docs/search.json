[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Legacy Project: Visual Acuity",
    "section": "",
    "text": "## About\nThis project aims to gather, curate, summarize, and publish data about visual acuity as measured by the Teller Acuity Card (TAC) procedure and described in the published literature. The project will summarize data based on participants grouped by age, sex/gender, or other characteristics. The project will also summarize individual participant data where available.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>About</span>"
    ]
  },
  {
    "objectID": "protocol_v01.html",
    "href": "protocol_v01.html",
    "title": "Protocol",
    "section": "",
    "text": "Overview\nThis page describes the protocol for collecting, capturing, and cleaning data associated with the project.\nThe text here is adapted from a working draft document written in Google Docs that is hosted here:\nhttps://docs.google.com/document/d/1-ZKrNflBO7UIq5I0pJ8U5K-rtoiads12Zf_OQq_S65Q/edit",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Protocol</span>"
    ]
  },
  {
    "objectID": "protocol_v01.html#overview",
    "href": "protocol_v01.html#overview",
    "title": "Protocol",
    "section": "",
    "text": "Warning\n\n\n\nOn April 1, no joke!, we changed the protocol substantially based on our discovery that Paperpile can export references with useful tags directly to our Github repo.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Protocol</span>"
    ]
  },
  {
    "objectID": "protocol_v01.html#literature-search",
    "href": "protocol_v01.html#literature-search",
    "title": "Protocol",
    "section": "Literature search",
    "text": "Literature search\n\nGoals\n\nCapture the published literature that reports data on using Teller Acuity Cards.\nCreate a database of that literature.\n\n\n\nPhase I: Search\n\n\n\n\n\n\nNote\n\n\n\nHere are some preparatory steps that need to be completed prior to doing literature search tasks.\n\nOpen Google Chrome from a lab computer.\nLog into psubrainlab’s google account (psubrainlab@gmail.com ask Dr. Gilmore or Andrea for password)\nSearch “paperpile chrome extension” and install the paperpile chrome extension\nOn the paperpile website click ’log in”. Choose the “log in with google” option\n\n\n\n\nSearch terms\nteller acuity cards visual acuity cards teller cards\n\n\nGoogle Scholar\n\nVisit Google Scholar: https://scholar.google.com/\nEnter the search term\nMake sure to select ‘Sort by relevance’, ‘Any type’, and do not check ‘include patents’ or ‘include citations’\n\nFor example, this URL uses teller acuity cards:\nhttps://scholar.google.com/scholar?hl=en&as_sdt=0,39&as_vis=1&q=%22teller+acuity+cards%22\nWhen looking for sources on Google Scholar, look for the Paperpile box and ensure it is gray. Gray sources are sources we do not currently have in Paperpile.\n\n\n\nReference not yet in our Paperpile\n\n\nGreen sources indicate we already have collected them in Paperpile.\n\n\n\nReference already in Paperpile\n\n\nAdd the references not already in Paperpile by pressing the grey button.\n\nAfter you have retrieved a page of entries, switch to the Paperpile tab to add the TAC tag to these entries. Use shift-click to select multiple entries in Paperpile.\n\n\n\n\nPapers recently added to Paperpile. After clicking on the tags button and entering ‘TAC’ in the search window, Paperpile lists all of the paper with the . ‘TAC’ tag. To add ‘TAC’ to the new, selected papers, click on the checkbox.\n\n\n\n\n\nPhase II: Data entry\n\nGoals\n\nCreate a database of references found in Phase I.\n\n\n\nOption A: Automated extraction\n\n\n\n\n\n\nNote\n\n\n\nIt is possible to export the Paperpile references with the TAC tag directly to Github. Rick tested this workflow on 2024-02-02. What is yet unclear is whether the reference file automatically updates.\nThere appear to be some R packages that can read *.bib files and convert them to data frames, see https://rdrr.io/cran/revtools/ and https://docs.ropensci.org/bib2df/.\n\n\nWe should assign someone the task of determining how to parse a *.bib file into a data frame.\n\n\nOption B: Manual entry\nIn the meantime, we fall back on manual data entry.\n\nOpen the Google Sheet “Legacy Project Acuity Data: By Paper” and open the paper_data tab:\n\nhttps://docs.google.com/spreadsheets/d/1UFZkbh9oU4JHpYsrkDQcNmDyqD4J-qB74dhyMzIkqKs/edit#gid=2144658778\n\n\n\n\n\n\nFigure 2.1: Paper reference database\n\n\n\n\nIn a second Chrome window, click on the Paperpile icon and open Paperpile.\n\n\n\n\n\n\n\nIn the left panel of the Paperpile window, search for the TAC tag and select it.\n\n\n\n\nSelecting the ‘TAC’ (Teller Acuity Cards) tag in Paperpile\n\n\n\nIn the right panel of the Paperpile window, select sort by author.\n\n\n\n\nSort Paperpile entries by author\n\n\n\nShrink the Paperpile window and push it flush right, then shrink the Google Sheets window and push it flush left.\n\n\n\n\nWindow placement for data entry\n\n\n\nSort Google Sheet data by Author\nCopy citation (in APA format) from Paperpile (using the command+c keyboard shortcut) and paste into ‘citation’ field in Google Sheet.\nReformat Google Sheet ‘citation’ column.\n\n\nSelect column\nSelect Format | Wrapping | Wrap\n\n\n\n\n\n\n\nSort Google Sheet data by author_first and pub_year\n\nData | Sort Range | Advanced\n\n\n\nSorting a Google Sheet\n\n\n\n\n\nSort settings for author_first and pub_year",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Protocol</span>"
    ]
  },
  {
    "objectID": "protocol_v01.html#data-evaluation-extraction",
    "href": "protocol_v01.html#data-evaluation-extraction",
    "title": "Protocol",
    "section": "Data evaluation & extraction",
    "text": "Data evaluation & extraction\n\nGoals\n\nTo evaluate each paper to determine whether it contains extractable data.\nTo extract group-level data from each paper identified as having extractable data.\nEnter group-level data into a common database.\n\n\n\nData evaluation\n\nPhase I\nFor each paper in the paper_data tab in the Google Sheet\nhttps://docs.google.com/spreadsheets/d/1UFZkbh9oU4JHpYsrkDQcNmDyqD4J-qB74dhyMzIkqKs/edit#gid=2144658778\n\nOpen the paper via its URL.\n\nAdd “yes” to the url_openable data column in the Google Sheet to report that you could open a PDF or web version of the complete paper.\nEnter “paywall” if the URL resolves, but you would need to pay for access to the paper.\nIf the paper can’t be found because the URL doesn’t resolve, enter “not_found”.\n\n\nPhase II\nSelect the papers that could be opened (url_openable == “yes”)\n\nScan the paper to find any data tables that contain acuity card summary data.\n\n\n\n\nData extraction\n\n\nQuality Assurance",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Protocol</span>"
    ]
  },
  {
    "objectID": "dashboard.html",
    "href": "dashboard.html",
    "title": "Dashboard",
    "section": "",
    "text": "Set-up\nWe load ggplot2 to make the following plot commands easier to type.\nCode\nlibrary(ggplot2)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Dashboard</span>"
    ]
  },
  {
    "objectID": "dashboard.html#download",
    "href": "dashboard.html#download",
    "title": "Dashboard",
    "section": "Download",
    "text": "Download\nThe data are stored in a Google sheet that we download again if params$update_data == TRUE. Otherwise, we make use of a stored data file.\n\n\nCode\nif (!dir.exists(params$data_dir)) {\n  message(\"Creating missing \",  params$data_dir, \".\")\n  dir.create(params$data_dir)\n}\n\nproject_ss &lt;- params$google_data_url\n\nif (params$update_data) {\n  if (params$use_sysenv_creds) {\n    google_creds &lt;- Sys.getenv(\"GMAIL_SURVEY\")\n    if (google_creds != \"\") {\n      options(gargle_oauth_email = google_creds)\n      googledrive::drive_auth()\n    } else {\n      message(\"No Google account information stored in `.Renviron`.\")\n      message(\"Add authorized Google account name to `.Renviron` using `usethist::edit_r_environ()`.\")\n    }\n  }\n\n  papers_data &lt;- googlesheets4::read_sheet(ss = project_ss,\n                            sheet = params$sheet_name)\n  out_fn &lt;- file.path(params$data_dir, params$data_fn)\n  readr::write_csv(papers_data, out_fn)\n  message(\"Data updated: \", out_fn)\n} else {\n  message(\"Using stored data.\")\n  papers_data &lt;- readr::read_csv(file.path(params$data_dir, params$data_fn),\n                                 show_col_types = FALSE)\n}\n\n\nUsing stored data.\n\n\n\nSynched Paperpile file from GitHub\nWe have configured Paperpile to synch a .bib formatted file directly with this repo on GitHub. The files can be found here: src/data/*paperpile*.bib.\nWe import data/paperpile-tac-has-pdf.bib and data/paperpile-tac-no-pdf.bib separately; add a variable indicating whether we have or do not have a PDF; then, join the two data data frames.\n\n\nCode\nrefs_w_pdf &lt;- bib2df::bib2df(\"data/paperpile-tac-has-pdf.bib\", separate_names = TRUE)\n\n\nSome BibTeX entries may have been dropped.\n            The result could be malformed.\n            Review the .bib file and make sure every single entry starts\n            with a '@'.\n\n\nColumn `YEAR` contains character strings.\n              No coercion to numeric applied.\n\n\nCode\nrefs_w_pdf &lt;- refs_w_pdf |&gt;\n  dplyr::mutate(pdf = TRUE)\n\n\nWe have 313 papers with PDFs to process.\n\n\nCode\nrefs_no_pdf &lt;- bib2df::bib2df(\"data/paperpile-tac-no-pdf.bib\", separate_names = TRUE)\n\n\nSome BibTeX entries may have been dropped.\n            The result could be malformed.\n            Review the .bib file and make sure every single entry starts\n            with a '@'.\n\n\nColumn `YEAR` contains character strings.\n              No coercion to numeric applied.\n\n\nCode\nrefs_no_pdf &lt;- refs_no_pdf |&gt;\n  dplyr::mutate(pdf = FALSE)\n\n\nWe have 438 papers without PDFs to process. In a separate workflow, we will try to access these papers via the PSU Libraries and other sources.\n\n\nCode\nrefs_all &lt;- dplyr::full_join(refs_w_pdf, refs_no_pdf)\n\n\nJoining with `by = join_by(CATEGORY, BIBTEXKEY, ADDRESS, ANNOTE, AUTHOR,\nBOOKTITLE, CHAPTER, CROSSREF, EDITION, EDITOR, HOWPUBLISHED, INSTITUTION,\nJOURNAL, KEY, MONTH, NOTE, NUMBER, ORGANIZATION, PAGES, PUBLISHER, SCHOOL,\nSERIES, TITLE, TYPE, VOLUME, YEAR, URL, DOI, PMC, PMID, ISSN, LANGUAGE,\nKEYWORDS, ISBN, pdf)`",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Dashboard</span>"
    ]
  },
  {
    "objectID": "dashboard.html#clean",
    "href": "dashboard.html#clean",
    "title": "Dashboard",
    "section": "Clean",
    "text": "Clean\nThe author and editor fields are imported as lists. We need to merge these into character strings to re-import the data back into Google Sheets.\n\n\nCode\n# Create function to change AUTHOR list to a string array\nmake_author_list &lt;- function(df) {\n  unlist(df$full_name) |&gt; paste(collapse = \"; \")\n}\n\nmake_editor_list &lt;- function(df) {\n  if (is.na(df$full_name)) {\n    \"\"\n  } else {\n  unlist(df$full_name) |&gt; paste(collapse = \"; \")    \n  }\n}\n\nauthors_string &lt;- purrr::map(refs_all$AUTHOR, make_author_list) |&gt; \n  purrr::list_c()\n\neditors_string &lt;- purrr::map(refs_all$EDITOR, make_author_list) |&gt; \n  purrr::list_c()\n\nnew_refs_all &lt;- refs_all |&gt;\n  dplyr::mutate(authors = authors_string,\n                editors = editors_string) |&gt;\n  dplyr::select(-c(\"AUTHOR\", \"ANNOTE\", \"EDITOR\"))",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Dashboard</span>"
    ]
  },
  {
    "objectID": "dashboard.html#upload-cleaned",
    "href": "dashboard.html#upload-cleaned",
    "title": "Dashboard",
    "section": "Upload cleaned",
    "text": "Upload cleaned\nWe the push the cleaned data back to Google Sheets for further analysis and processing.\n\n\n\n\n\n\nWarning\n\n\n\nWe do not push the cleaned data back to the original sheet but to a new one to avoid overwriting data.\n\n\n\n\nCode\nnew_refs_all |&gt; \n  googlesheets4::sheet_write(project_ss, \"from_paperpile_via_github_cleaned\")\n\n\n! Using an auto-discovered, cached token.\n\n\n  To suppress this message, modify your code or options to clearly consent to\n  the use of a cached token.\n\n\n  See gargle's \"Non-interactive auth\" vignette for more details:\n\n\n  &lt;https://gargle.r-lib.org/articles/non-interactive-auth.html&gt;\n\n\nℹ The googlesheets4 package is using a cached token for\n  'rick.o.gilmore@gmail.com'.\n\n\n✔ Writing to \"Legacy Project Acuity Data: By Paper\".\n\n\n✔ Writing to sheet 'from_paperpile_via_github_cleaned'.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Dashboard</span>"
    ]
  },
  {
    "objectID": "dashboard.html#visualize",
    "href": "dashboard.html#visualize",
    "title": "Dashboard",
    "section": "Visualize",
    "text": "Visualize\n\nPapers by publication date\nThe following uses the new has-pdf/no-pdf export workflow from Paperpile directly to Github.\n\n\nCode\nrefs_all |&gt;\n  ggplot() +\n  aes(x = YEAR, fill = pdf) +\n  geom_bar() +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n\n\n\n\n\n\n\nFigure 3.1: Papers by publication year\n\n\n\n\n\nThere are 751 papers in our Paperpile. Of these 313 have PDFs.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Dashboard</span>"
    ]
  },
  {
    "objectID": "dashboard.html#extracted-tables",
    "href": "dashboard.html#extracted-tables",
    "title": "Dashboard",
    "section": "Extracted tables",
    "text": "Extracted tables\nThis section extracts data about our progress in capturing data tables from these articles.\n\n\nCode\nimg_folder &lt;- googledrive::drive_find(type = \"folder\", q = \"name contains 'legacy'\")\n\n\nℹ Suitable tokens found in the cache, associated with these emails:\n\n\n• 'psubrainlab@gmail.com'\n\n\n• 'rick.o.gilmore@gmail.com'\n\n\n  Defaulting to the first email.\n\n\n! Using an auto-discovered, cached token.\n\n\n  To suppress this message, modify your code or options to clearly consent to\n  the use of a cached token.\n\n\n  See gargle's \"Non-interactive auth\" vignette for more details:\n\n\n  &lt;https://gargle.r-lib.org/articles/non-interactive-auth.html&gt;\n\n\nℹ The googledrive package is using a cached token for 'psubrainlab@gmail.com'.\n\n\nCode\nimg_df &lt;- googledrive::drive_ls(img_folder)\n\nimg_df &lt;- img_df |&gt;\n  dplyr::mutate(paper_id = stringr::str_extract_all(name, \"[a-zA-Z0-9]+\\\\-[a-z]{2}\"))\n\n\nNow that we have re-extracted the paper_id, we can do some summaries.\n\n\nCode\nn_tables &lt;- dim(img_df)[1]\nn_papers &lt;- length(unique(img_df$paper_id))\n\n\nWe have processed 81 papers and 233 tables as of 2024-09-05 07:47:50.588578.\n\nPapers entered by analyst\nWe use the from_paperpile_via_github tab to keep track of our work. So, we first import this sheet.\n\n\nCode\npapers_progress_data &lt;- googlesheets4::read_sheet(ss = project_ss,\n                            sheet = \"from_paperpile_via_github\")\n\n\n✔ Reading from \"Legacy Project Acuity Data: By Paper\".\n\n\n✔ Range ''from_paperpile_via_github''.\n\n\nHere is a table of the papers processed by each analyst.\n\n\nCode\nxtabs(formula = ~ open_attempt_by, data = papers_progress_data)\n\n\nopen_attempt_by\nbhb jmd nlc \n 74  27   4 \n\n\nHere is a table of the number of captured figures:\n\n\n\n\n\n\nWarning\n\n\n\nAs of 2024-09-05, we do not render this table because number_of_captured_figs is a non-numeric list.\n\n\n\n\nCode\n# 2024-09-05 do not evaluate because number_of_captured_figs is a non-numeric list\npapers_progress_data |&gt;\n  dplyr::filter(!is.na(number_of_captured_figs),\n                !is.na(open_attempt_by)) |&gt;\n  dplyr::group_by(open_attempt_by) |&gt;\n  dplyr::summarise(n_figs = sum(number_of_captured_figs)) |&gt;\n  knitr::kable(\"html\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Dashboard</span>"
    ]
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Overview",
    "section": "",
    "text": "Gathering\nWe use a Google Sheet to store the by-study data:\nhttps://docs.google.com/spreadsheets/d/1UFZkbh9oU4JHpYsrkDQcNmDyqD4J-qB74dhyMzIkqKs/edit#gid=0\nThe googledrive package provides a convenient way to access documents stored on Google.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cleaning & visualization</span>"
    ]
  },
  {
    "objectID": "data.html#gathering",
    "href": "data.html#gathering",
    "title": "Overview",
    "section": "",
    "text": "Note\n\n\n\nNote: There is no identifiable data here at the moment, so Google Sheets are a viable option.\nLater on, we start contacting authors, we will need to restrict access to that information for privacy reasons.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWe need a process for managing who has edit access.\n\n\n\n\nDownload from Google as CSV\n\n\nCode\nif (!dir.exists(params$data_dir)) {\n  message(\"Creating missing \",  params$data_dir, \".\")\n  dir.create(params$data_dir)\n}\n\nif (params$update_data) {\n  if (params$use_sysenv_creds) {\n    google_creds &lt;- Sys.getenv(\"GMAIL_SURVEY\")\n    if (google_creds != \"\") {\n      options(gargle_oauth_email = google_creds)\n      googledrive::drive_auth()\n    } else {\n      message(\"No Google account information stored in `.Renviron`.\")\n      message(\"Add authorized Google account name to `.Renviron` using `usethist::edit_r_environ()`.\")\n    }\n  }\n\n  this_sheet &lt;- googlesheets4::read_sheet(ss = params$google_data_url,\n                            sheet = params$sheet_name)\n  out_fn &lt;- file.path(params$data_dir, params$data_fn)\n  readr::write_csv(this_sheet, out_fn)\n  message(\"Data updated: \", out_fn)\n} else {\n  message(\"Using stored data.\")\n}\n\n\n✔ Reading from \"Legacy Project Acuity Data: By Paper\".\n\n\n✔ Range ''by_paper''.\n\n\nData updated: data/csv/by-paper.csv\n\n\nThe data file has been saved as a comma-separated value (CSV) format data file in a special directory called csv/.\n\n\nOpen CSV\nNext we load the data file.\n\n\nCode\nacuity_df &lt;-\n  readr::read_csv(file.path(params$data_dir, \"by-paper.csv\"), show_col_types = FALSE)\n\n\nWe’ll show the column (variable names) since these will be part of our data dictionary.\n\n\nCode\nacuity_cols &lt;- names(acuity_df)\nacuity_cols\n\n\n [1] \"author_first\"         \"citation\"             \"pub_year\"            \n [4] \"fig_table\"            \"age_mos\"              \"age_grp_rog\"         \n [7] \"binoc_monoc\"          \"n_participants\"       \"distance_cm\"         \n[10] \"start_card_cyc_deg\"   \"mean_acuity_cyc_deg\"  \"lower_limit_cyc_deg\" \n[13] \"closest_card_cyc_deg\" \"upper_limit_cyc_deg\"  \"country\"             \n[16] \"card_type\"           \n\n\n\n\nCreate data dictionary\nWe’ll start by creating a data dictionary so that we can refer to it later in our cleaning and data analysis. We do this by creating a data frame or ‘tibble’ because this is a convenient format for manipulating the information.\n\n\nCode\nacuity_data_dict &lt;- tibble::tibble(col_name = names(acuity_df))\n\n\nNow, we write a short description of each variable in the data file.\n\n\nCode\nacuity_data_dict &lt;- acuity_data_dict |&gt;\n  dplyr::mutate(col_desc = c(\"Last name of 1st author\",\n                             \"Full APA format citation\",\n                             \"Paper publication year\",\n                             \"Source in paper\",\n                             \"Reported age range in mos\",\n                             \"Age in mos as conformed by ROG\",\n                             \"Participants tested monocularly or binocularly\",\n                             \"Number of participants in group\",\n                             \"Testing distance in cm\",\n                             \"Starting card in cyc/deg\",\n                             \"Mean (group) acuity in cyc/deg\",\n                             \"Estimated lower limit of acuity in cyc/deg\",\n                             \"Teller Acuity Card closest equivalent to this lower limit\",\n                             \"Estimated upper limit of acuity in cyc/deg\",\n                             \"Country where data were collected\",\n                             \"TAC-I or TAC-II\"))\n\nacuity_data_dict |&gt;\n  knitr::kable(format = 'html') |&gt;\n  kableExtra::kable_classic()\n\n\n\n\n\ncol_name\ncol_desc\n\n\n\n\nauthor_first\nLast name of 1st author\n\n\ncitation\nFull APA format citation\n\n\npub_year\nPaper publication year\n\n\nfig_table\nSource in paper\n\n\nage_mos\nReported age range in mos\n\n\nage_grp_rog\nAge in mos as conformed by ROG\n\n\nbinoc_monoc\nParticipants tested monocularly or binocularly\n\n\nn_participants\nNumber of participants in group\n\n\ndistance_cm\nTesting distance in cm\n\n\nstart_card_cyc_deg\nStarting card in cyc/deg\n\n\nmean_acuity_cyc_deg\nMean (group) acuity in cyc/deg\n\n\nlower_limit_cyc_deg\nEstimated lower limit of acuity in cyc/deg\n\n\nclosest_card_cyc_deg\nTeller Acuity Card closest equivalent to this lower limit\n\n\nupper_limit_cyc_deg\nEstimated upper limit of acuity in cyc/deg\n\n\ncountry\nCountry where data were collected\n\n\ncard_type\nTAC-I or TAC-II",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cleaning & visualization</span>"
    ]
  },
  {
    "objectID": "data.html#data-visualization",
    "href": "data.html#data-visualization",
    "title": "Overview",
    "section": "Data visualization",
    "text": "Data visualization\n\n\n\n\n\n\nImportant\n\n\n\nRick Gilmore decided to take the mean of the age range reported in the (Xiang et al., 2021) data and create a new variable strictly for visualization purposes, age_grp_rog.\n\n\nWe are still in the early phases of the project (as of 2024-09-05 07:47:55.13516), but it is good to start sketching the the data visualizations we will eventually want to see.\n\n\nCode\nlibrary(ggplot2)\nacuity_df |&gt;\n  ggplot() +\n  aes(x = age_grp_rog, y = mean_acuity_cyc_deg, color = country) +\n  geom_point() +\n  geom_smooth() +\n  facet_grid(cols = vars(binoc_monoc))\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning in simpleLoess(y, x, w, span, degree = degree, parametric = parametric,\n: pseudoinverse used at 3\n\n\nWarning in simpleLoess(y, x, w, span, degree = degree, parametric = parametric,\n: neighborhood radius 3\n\n\nWarning in simpleLoess(y, x, w, span, degree = degree, parametric = parametric,\n: reciprocal condition number 4.2872e-17\n\n\nWarning in predLoess(object$y, object$x, newx = if (is.null(newdata)) object$x\nelse if (is.data.frame(newdata))\nas.matrix(model.frame(delete.response(terms(object)), : pseudoinverse used at 3\n\n\nWarning in predLoess(object$y, object$x, newx = if (is.null(newdata)) object$x\nelse if (is.data.frame(newdata))\nas.matrix(model.frame(delete.response(terms(object)), : neighborhood radius 3\n\n\nWarning in predLoess(object$y, object$x, newx = if (is.null(newdata)) object$x\nelse if (is.data.frame(newdata))\nas.matrix(model.frame(delete.response(terms(object)), : reciprocal condition\nnumber 4.2872e-17\n\n\n\n\n\n\n\n\nFigure 4.1: Developmental time course of mean acuity as assessed by Teller Acuity Cards",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cleaning & visualization</span>"
    ]
  },
  {
    "objectID": "data.html#by-individual-data",
    "href": "data.html#by-individual-data",
    "title": "Overview",
    "section": "By-individual data",
    "text": "By-individual data\nThe Gilmore lab has some archival data that we can potentially use in this project. The following represents Rick Gilmore’s work to gather, clean, and visualize these data.\n\nGathering\nThe de-identified archival data are stored in a Google sheet accessed by the lab Google account.\nFirst, we must authenticate to Google to access the relevant file and download it.\n\n\nCode\noptions(gargle_oauth_email = \"psubrainlab@gmail.com\")\ngoogledrive::drive_auth()\n\n\nThen we download the relevant file.\n\n\nCode\ngoogledrive::drive_download(\n  \"vep-session-log\",\n  path = file.path(params$data_dir, \"by-participant.csv\"),\n  type = \"csv\",\n  overwrite = TRUE\n)\n\n\nFile downloaded:\n\n\n• 'vep-session-log' &lt;id: 1Y9GoJU6EFUxcNmbWOLr52enpkuCyg_wSFv3pA-SWm4Q&gt;\n\n\nSaved locally as:\n\n\n• 'data/csv/by-participant.csv'\n\n\nUnlike the Google sheet newly created for the by-study data, this one requires a lot of cleaning.\n\n\nCode\ngilmore_archival_df &lt;-\n  readr::read_csv(file.path(params$data_dir, \"by-participant.csv\"),\n                  show_col_types = FALSE)\nnames(gilmore_archival_df)\n\n\n [1] \"Participant Number in an IRB year\"  \"IRB Count\"                         \n [3] \"Date\"                               \"Time\"                              \n [5] \"Session-Leader\"                     \"Sex\"                               \n [7] \"Participant-ID\"                     \"DOB\"                               \n [9] \"infant\"                             \"Stimset-name\"                      \n[11] \"Backed up to project folder on Box\" \"Export-Status\"                     \n[13] \"Snellen-Acuity\"                     \"Teller Acuity Cards\"               \n[15] \"Stereo-Acuity\"                      \"Zipped on Drive\"                   \n[17] \"Age at test\"                        \"keep-session\"                      \n[19] \"Comments\"                          \n\n\nWe’ll keep Date, Time, Sex, DOB, Teller Acuity Cards, Age at test.\n\n\nCode\ngilmore_archival_df &lt;- gilmore_archival_df |&gt;\n  dplyr::select(Date, Time, Sex, DOB, `Teller Acuity Cards`, `Age at test`)\n\n\nThen, let’s filter those where we have TAC data.\n\n\nCode\nwith(gilmore_archival_df, unique(`Teller Acuity Cards`))\n\n\n [1] \"20/94 6.4 cyc/deg @ 55cm\"                                   \n [2] \"20/190 3.1 cyc/deg @ 55cm\"                                  \n [3] \"20/470 1.3 cyc/deg @ 55cm\"                                  \n [4] \"20/380 1.6 cyc/deg @ 55cm - difficulty attending toward end\"\n [5] \"20/130 4.7 cyc/deg @ 55cm\"                                  \n [6] NA                                                           \n [7] \"not interested\"                                             \n [8] \"20/94 6.54 cyc/deg @ 55cm\"                                  \n [9] \"20/130 4/7 cyc/deg @ 55cm\"                                  \n[10] \"20/94 6.40 cyc/deg @ 55cm\"                                  \n[11] \"20/170 3.6 cyc/deg @84cm\"                                   \n[12] \"20/190 3.1 cyc/deg @55cm\"                                   \n[13] \"20/94 6.5 cyc/deg @ 55cm\"                                   \n\n\n\n\nCode\ngilmore_archival_df &lt;- gilmore_archival_df |&gt;\n  dplyr::filter(!is.na(`Teller Acuity Cards`),\n                `Teller Acuity Cards` != \"not interested\")\n\ndim(gilmore_archival_df)\n\n\n[1] 34  6\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis file illustrates how making data FAIR from the outset can save work.\nThis one is not too terribly hard to parse, but it could have been better planned.\n\n\nWe’ll extract the viewing distance with a regular expression.\n\n\nCode\ngilmore_archival_df &lt;- gilmore_archival_df |&gt;\n  dplyr::mutate(view_dist_cm = stringr::str_extract(`Teller Acuity Cards`, \"[0-9]{2}cm\")) |&gt;\n  dplyr::mutate(view_dist_cm = stringr::str_remove(view_dist_cm, \"cm\")) # remove 'cm'\ngilmore_archival_df$view_dist_cm\n\n\n [1] \"55\" \"55\" \"55\" \"55\" \"55\" \"55\" \"55\" \"55\" \"55\" \"55\" \"55\" \"55\" \"55\" \"55\" \"55\"\n[16] \"55\" \"55\" \"55\" \"55\" \"55\" \"55\" \"55\" \"55\" \"55\" \"55\" \"55\" \"55\" \"55\" \"55\" \"84\"\n[31] \"55\" \"55\" \"55\" \"55\"\n\n\nSimilarly, we’ll extract the acuity in cyc/deg using a regular expression.\n\n\nCode\ngilmore_archival_df &lt;- gilmore_archival_df |&gt;\n  # add 'cyc' to separate cyc/deg from Snellen acuity\n  dplyr::mutate(acuity_cyc_deg = stringr::str_extract(`Teller Acuity Cards`, \"[0-9]{1}[\\\\./]{1}[0-9]+ cyc\")) |&gt;\n  dplyr::mutate(acuity_cyc_deg = stringr::str_remove(acuity_cyc_deg, \" cyc\")) |&gt;\n  dplyr::mutate(acuity_cyc_deg = stringr::str_replace(acuity_cyc_deg, \"/\", \".\"))\n\ngilmore_archival_df$acuity_cyc_deg\n\n\n [1] \"6.4\"  \"3.1\"  \"1.3\"  \"3.1\"  \"3.1\"  \"1.6\"  \"4.7\"  \"3.1\"  \"3.1\"  \"3.1\" \n[11] \"6.4\"  \"4.7\"  \"3.1\"  \"4.7\"  \"3.1\"  \"4.7\"  \"4.7\"  \"6.4\"  \"4.7\"  \"4.7\" \n[21] \"6.54\" \"4.7\"  \"4.7\"  \"6.40\" \"4.7\"  \"4.7\"  \"4.7\"  \"3.1\"  \"3.1\"  \"3.6\" \n[31] \"4.7\"  \"3.1\"  \"6.5\"  \"3.1\" \n\n\nNow, let’s look at the age at test.\n\n\nCode\ngilmore_archival_df$`Age at test`\n\n\n [1] \"0 Years, 6 Months, 25 Days\" \"0 Years, 5 Months, 28 Days\"\n [3] \"0 Years, 5 Months, 8 Days\"  \"0 Years, 6 Months, 14 Days\"\n [5] \"0 Years, 4 Months, 27 Days\" \"0 Years, 4 Months, 2 Days\" \n [7] \"0 Years, 4 Months, 21 Days\" \"0 Years, 4 Months, 27 Days\"\n [9] \"0 Years, 3 Months, 7 Days\"  \"0 Years, 4 Months, 10 Days\"\n[11] \"0 Years, 6 Months, 24 Days\" \"0 Years, 7 Months, 26 Days\"\n[13] \"0 Years, 8 Months, 8 Days\"  \"0 Years, 8 Months, 21 Days\"\n[15] \"0 Years, 8 Months, 2 Days\"  \"0 Years, 8 Months, 2 Days\" \n[17] \"0 Years, 7 Months, 18 Days\" \"0 Years, 7 Months, 27 Days\"\n[19] \"0 Years, 8 Months, 7 Days\"  \"0 Years, 7 Months, 12 Days\"\n[21] \"0 Years, 5 Months, 26 Days\" \"0 Years, 8 Months, 3 Days\" \n[23] \"0 Years, 5 Months, 18 Days\" \"0 Years, 6 Months, 25 Days\"\n[25] \"0 Years, 8 Months, 14 Days\" \"0 Years, 4 Months, 17 Days\"\n[27] \"0 Years, 8 Months, 7 Days\"  \"0 Years, 8 Months, 7 Days\" \n[29] \"0 Years, 8 Months, 0 Days\"  \"0 Years, 6 Months, 2 Days\" \n[31] \"0 Years, 7 Months, 29 Days\" \"0 Years, 7 Months, 13 Days\"\n[33] \"0 Years, 6 Months, 5 Days\"  \"0 Years, 5 Months, 20 Days\"\n\n\nInstead, let’s see what it looks like to compute age at test from the dates.\n\n\nCode\ngilmore_archival_df &lt;- gilmore_archival_df |&gt;\n  dplyr::mutate(age_at_test_days = lubridate::mdy(Date) - lubridate::mdy(DOB))\n\ngilmore_archival_df$age_at_test_days\n\n\nTime differences in days\n [1] 208 181 161 198 149 125 143 149  99 132 207 239 252 265 245 245 231 242 252\n[20] 225 179 247 171 209 259 139 249 250 243 186 241 225 186 171\n\n\nThat seems reasonable for now.\nLet’s see if we can plot these data.\n\n\nCode\ngilmore_archival_df |&gt;\n  dplyr::mutate(acuity_cyc_deg = as.numeric(acuity_cyc_deg)) |&gt;\n  ggplot() +\n  aes(x = age_at_test_days, y = acuity_cyc_deg, color = Sex) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  #theme_classic() +\n  theme(legend.position = \"bottom\", legend.title = element_blank()) \n\n\nDon't know how to automatically pick scale for object of type &lt;difftime&gt;.\nDefaulting to continuous.\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nFigure 4.2: Individual participant Teller Acuity Card thresholds from archival Gilmore lab data\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nBefore I stop, I’m going to add the by-participant data file to a .gitignore file, just to be extra careful.\n\n\n\n\n\n\n\n\nXiang, Y., Long, E., Liu, Z., Li, X., Lin, Z., Zhu, Y., … Lin, H. (2021). Study to establish visual acuity norms with teller acuity cards II for infants from southern china. Eye, 35(10), 2787–2792. https://doi.org/10.1038/s41433-020-01314-y",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cleaning & visualization</span>"
    ]
  },
  {
    "objectID": "data-management-plan.html",
    "href": "data-management-plan.html",
    "title": "Data Management Plan",
    "section": "",
    "text": "Data Management and Sharing Plan\nIf any of the proposed research in the application involves the generation of scientific data, this application is subject to the NIH Policy for Data Management and Sharing and requires submission of a Data Management and Sharing Plan. If the proposed research in the application will generate large-scale genomic data, the Genomic Data Sharing Policy also applies and should be addressed in this Plan. Refer to the detailed instructions in the application guide for developing this plan as well as to additional guidance on https://sharing.nih.gov. The Plan is recommended not to exceed two pages. Text in italics should be deleted. There is no “form page” for the Data Management and Sharing Plan. The DMS Plan may be provided in the format shown below.\nPublic reporting burden for this collection of information is estimated to average 2 hours per response, including the time for reviewing instructions, searching existing data sources, gathering, and maintaining the data needed, and completing and reviewing the collection of information. An agency may not conduct or sponsor, and a person is not required to respond to, a collection of information unless it displays a currently valid OMB control number. Send comments regarding this burden estimate or any other aspect of this collection of information, including suggestions for reducing this burden, to: NIH, Project Clearance Branch, 6705 Rockledge Drive, MSC 7974, Bethesda, MD 20892-7974, ATTN: PRA (0925-0001 and 0925-0002). Do not return the completed form to this address.",
    "crumbs": [
      "Addenda",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Management Plan</span>"
    ]
  },
  {
    "objectID": "data-management-plan.html#element-1-data-type",
    "href": "data-management-plan.html#element-1-data-type",
    "title": "Data Management Plan",
    "section": "Element 1: Data Type",
    "text": "Element 1: Data Type\n\nA. Types and amount of scientific data expected to be generated in the project:\nSummarize the types and estimated amount of scientific data expected to be generated in the project.\n\n\nB. Scientific data that will be preserved and shared, and the rationale for doing so:\nDescribe which scientific data from the project will be preserved and shared and provide the rationale for this decision.\n\n\nC. Metadata, other relevant data, and associated documentation:\nBriefly list the metadata, other relevant data, and any associated documentation (e.g., study protocols and data collection instruments) that will be made accessible to facilitate interpretation of the scientific data.\nThe full protocol, including survey questions is shared publicly via a website on the code/documentation sharing website, GitHub.",
    "crumbs": [
      "Addenda",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Management Plan</span>"
    ]
  },
  {
    "objectID": "data-management-plan.html#element-2-related-tools-software-andor-code",
    "href": "data-management-plan.html#element-2-related-tools-software-andor-code",
    "title": "Data Management Plan",
    "section": "Element 2: Related Tools, Software and/or Code:",
    "text": "Element 2: Related Tools, Software and/or Code:\nState whether specialized tools, software, and/or code are needed to access or manipulate shared scientific data, and if so, provide the name(s) of the needed tool(s) and software and specify how they can be accessed.",
    "crumbs": [
      "Addenda",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Management Plan</span>"
    ]
  },
  {
    "objectID": "data-management-plan.html#element-3-standards",
    "href": "data-management-plan.html#element-3-standards",
    "title": "Data Management Plan",
    "section": "Element 3: Standards",
    "text": "Element 3: Standards\nState what common data standards will be applied to the scientific data and associated metadata to enable interoperability of datasets and resources, and provide the name(s) of the data standards that will be applied and describe how these data standards will be applied to the scientific data generated by the research proposed in this project. If applicable, indicate that no consensus standards exist.\nThere are no consensus standards that apply to these data to our knowledge. However, our publicly shared data cleaning code documents that ways that we modified the original dataset to make it more human-readable. CSV text files are considered to be interoperable.",
    "crumbs": [
      "Addenda",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Management Plan</span>"
    ]
  },
  {
    "objectID": "data-management-plan.html#element-4-data-preservation-access-and-associated-timelines",
    "href": "data-management-plan.html#element-4-data-preservation-access-and-associated-timelines",
    "title": "Data Management Plan",
    "section": "Element 4: Data Preservation, Access, and Associated Timelines",
    "text": "Element 4: Data Preservation, Access, and Associated Timelines\n\nA. Repository where scientific data and metadata will be archived:\nProvide the name of the repository(ies) where scientific data and metadata arising from the project will be archived; see Selecting a Data Repository.\nWhen the project is complete and we receive permission to do so, the data may be archived on Databrary (https://databrary.org). Metadata and summary data/visualizations will be archived on GitHub, https://gilmore-lab.github.io/visual-acuity/.\n\n\nB. How scientific data will be findable and identifiable:\nDescribe how the scientific data will be findable and identifiable, i.e., via a persistent unique identifier or other standard indexing tools.\nIf the project data are shared publicly, Databrary will create a persistent identifier (e.g., DOI).\n\n\nC. When and how long the scientific data will be made available:\nDescribe when the scientific data will be made available to other users (i.e., no later than time of an associated publication or end of the performance period, whichever comes first) and for how long data will be available.",
    "crumbs": [
      "Addenda",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Management Plan</span>"
    ]
  },
  {
    "objectID": "data-management-plan.html#element-5-access-distribution-or-reuse-considerations",
    "href": "data-management-plan.html#element-5-access-distribution-or-reuse-considerations",
    "title": "Data Management Plan",
    "section": "Element 5: Access, Distribution, or Reuse Considerations:",
    "text": "Element 5: Access, Distribution, or Reuse Considerations:\n\nA. Factors affecting subsequent access, distribution, or reuse of scientific data:\nNIH expects that in drafting Plans, researchers maximize the appropriate sharing of scientific data. Describe and justify any applicable factors or data use limitations affecting subsequent access, distribution, or reuse of scientific data related to informed consent, privacy and confidentiality protections, and any other considerations that may limit the extent of data sharing. See Frequently Asked Questions for examples of justifiable reasons for limiting sharing of data.\n\n\nB. Whether access to scientific data will be controlled:\nState whether access to the scientific data will be controlled (i.e., made available by a data repository only after approval).\nDatabrary does not approve data shared by researchers. Authorization to upload and share data is governed by a formal Databrary Access Agreement that binds Databrary and New York University with a researcher’s home institution. The DAA gives institutionally authorized researchers the right to share data and materials with Databrary consistent with the policies of their institution, ethics board approvals, and the permission of research participants. Similarly, while researchers can share data on Databrary and control or approve individual access themselves, Databrary does not require it.\nOnce shared, data from this project will be available to anyone.\n\n\nC. Protections for privacy, rights, and confidentiality of human research participants:\nIf generating scientific data derived from humans, describe how the privacy, rights, and confidentiality of human research participants will be protected (e.g., through de-identification, Certificates of Confidentiality, and other protective measures).",
    "crumbs": [
      "Addenda",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Management Plan</span>"
    ]
  },
  {
    "objectID": "data-management-plan.html#element-6-oversight-of-data-management-and-sharing",
    "href": "data-management-plan.html#element-6-oversight-of-data-management-and-sharing",
    "title": "Data Management Plan",
    "section": "Element 6: Oversight of Data Management and Sharing:",
    "text": "Element 6: Oversight of Data Management and Sharing:\nDescribe how compliance with this Plan will be monitored and managed, frequency of oversight, and by whom at your institution (e.g., titles, roles).\nThe PIs on the project will manage these provisions.",
    "crumbs": [
      "Addenda",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Management Plan</span>"
    ]
  }
]