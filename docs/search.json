[
  {
    "objectID": "protocol.html",
    "href": "protocol.html",
    "title": "Protocol",
    "section": "",
    "text": "This page describes the protocol for collecting, capturing, and cleaning data associated with the project.\nThe text here is adapted from a working draft document written in Google Docs that is hosted here:\nhttps://docs.google.com/document/d/1-ZKrNflBO7UIq5I0pJ8U5K-rtoiads12Zf_OQq_S65Q/edit"
  },
  {
    "objectID": "protocol.html#overview",
    "href": "protocol.html#overview",
    "title": "Protocol",
    "section": "",
    "text": "This page describes the protocol for collecting, capturing, and cleaning data associated with the project.\nThe text here is adapted from a working draft document written in Google Docs that is hosted here:\nhttps://docs.google.com/document/d/1-ZKrNflBO7UIq5I0pJ8U5K-rtoiads12Zf_OQq_S65Q/edit"
  },
  {
    "objectID": "protocol.html#literature-search",
    "href": "protocol.html#literature-search",
    "title": "Protocol",
    "section": "Literature search",
    "text": "Literature search\n\nGoals\n\nCapture the published literature that reports data on using Teller Acuity Cards.\nCreate a database of that literature.\n\n\n\nPhase I: Search\n\n\n\n\n\n\nNote\n\n\n\nHere are some preparatory steps that need to be completed prior to doing literature search tasks.\n\nOpen Google Chrome from a lab computer.\nLog into psubrainlab’s google account (psubrainlab@gmail.com ask Dr. Gilmore or Andrea for password)\nSearch “paperpile chrome extension” and install the paperpile chrome extension\nOn the paperpile website click ’log in”. Choose the “log in with google” option\n\n\n\n\nSearch terms\nteller acuity cards visual acuity cards teller cards\n\n\nGoogle Scholar\n\nVisit Google Scholar: https://scholar.google.com/\nEnter the search term\nMake sure to select ‘Sort by relevance’, ‘Any type’, and do not check ‘include patents’ or ‘include citations’\n\nFor example, this URL uses teller acuity cards:\nhttps://scholar.google.com/scholar?hl=en&as_sdt=0,39&as_vis=1&q=%22teller+acuity+cards%22\nWhen looking for sources on Google Scholar, look for the Paperpile box and ensure it is gray. Gray sources are sources we do not currently have in Paperpile.\n\n\n\nReference not yet in our Paperpile\n\n\nGreen sources indicate we already have collected them in Paperpile.\n\n\n\nReference already in Paperpile\n\n\nAdd the references not already in Paperpile by pressing the grey button.\n\nAfter you have retrieved a page of entries, switch to the Paperpile tab to add the TAC tag to these entries. Use shift-click to select multiple entries in Paperpile.\n\n\n\n\nPapers recently added to Paperpile. After clicking on the tags button and entering ‘TAC’ in the search window, Paperpile lists all of the paper with the . ‘TAC’ tag. To add ‘TAC’ to the new, selected papers, click on the checkbox.\n\n\n\n\n\nPhase II: Data entry\n\nGoals\n\nCreate a database of references found in Phase I.\n\n\n\nOption A: Automated extraction\n\n\n\n\n\n\nNote\n\n\n\nIt is possible to export the Paperpile references with the TAC tag directly to Github. Rick tested this workflow on 2024-02-02. What is yet unclear is whether the reference file automatically updates.\nThere appear to be some R packages that can read *.bib files and convert them to data frames, see https://rdrr.io/cran/revtools/ and https://docs.ropensci.org/bib2df/.\n\n\nWe should assign someone the task of determining how to parse a *.bib file into a data frame.\n\n\nOption B: Manual entry\nIn the meantime, we fall back on manual data entry.\n\nOpen the Google Sheet “Legacy Project Acuity Data: By Paper” and open the paper_data tab:\n\nhttps://docs.google.com/spreadsheets/d/1UFZkbh9oU4JHpYsrkDQcNmDyqD4J-qB74dhyMzIkqKs/edit#gid=2144658778\n\n\n\n\n\n\nFigure 1: Paper reference database\n\n\n\n\nIn a second Chrome window, click on the Paperpile icon and open Paperpile.\n\n\n\n\n\n\n\nIn the left panel of the Paperpile window, search for the TAC tag and select it.\n\n\n\n\nSelecting the ‘TAC’ (Teller Acuity Cards) tag in Paperpile\n\n\n\nIn the right panel of the Paperpile window, select sort by author.\n\n\n\n\nSort Paperpile entries by author\n\n\n\nShrink the Paperpile window and push it flush right, then shrink the Google Sheets window and push it flush left.\n\n\n\n\nWindow placement for data entry\n\n\n\nSort Google Sheet data by Author\nCopy citation (in APA format) from Paperpile (using the command+c keyboard shortcut) and paste into ‘citation’ field in Google Sheet.\nReformat Google Sheet ‘citation’ column.\n\n\nSelect column\nSelect Format | Wrapping | Wrap\n\n\n\n\n\n\n\nSort Google Sheet data by author_first and pub_year\n\nData | Sort Range | Advanced\n\n\n\nSorting a Google Sheet\n\n\n\n\n\nSort settings for author_first and pub_year"
  },
  {
    "objectID": "protocol.html#data-evaluation-extraction",
    "href": "protocol.html#data-evaluation-extraction",
    "title": "Protocol",
    "section": "Data evaluation & extraction",
    "text": "Data evaluation & extraction\n\nGoals\n\nTo evaluate each paper to determine whether it contains extractable data.\nTo extract group-level data from each paper identified as having extractable data.\nEnter group-level data into a common database.\n\n\n\nData evaluation\n\nPhase I\nFor each paper in the paper_data tab in the Google Sheet\nhttps://docs.google.com/spreadsheets/d/1UFZkbh9oU4JHpYsrkDQcNmDyqD4J-qB74dhyMzIkqKs/edit#gid=2144658778\n\nOpen the paper via its URL.\n\nAdd “yes” to the url_openable data column in the Google Sheet to report that you could open a PDF or web version of the complete paper.\nEnter “paywall” if the URL resolves, but you would need to pay for access to the paper.\nIf the paper can’t be found because the URL doesn’t resolve, enter “not_found”.\n\n\nPhase II\nSelect the papers that could be opened (url_openable == “yes”)\n\nScan the paper to find any data tables that contain acuity card summary data.\n\n\n\n\nData extraction\n\n\nQuality Assurance\n\n\n\nReference not yet in our Paperpile\nReference already in Paperpile\nPapers recently added to Paperpile. After clicking on the tags button and entering ‘TAC’ in the search window, Paperpile lists all of the paper with the . ‘TAC’ tag. To add ‘TAC’ to the new, selected papers, click on the checkbox.\nFigure 1: Paper reference database\nSelecting the ‘TAC’ (Teller Acuity Cards) tag in Paperpile\nSort Paperpile entries by author\nWindow placement for data entry\nSorting a Google Sheet\nSort settings for author_first and pub_year"
  },
  {
    "objectID": "index.html#investigators",
    "href": "index.html#investigators",
    "title": "Legacy Project: Visual acuity",
    "section": "Investigators",
    "text": "Investigators\nRick Gilmore is Professor of Psychology at Penn State.\nAndrea Seisler is the lab manager. She also serves as the Authorizations and Support Specialist for Databrary.\nBrianna Beamer is an undergraduate research assistant.\nNicole Cruz is an undergraduate research assistant.\nJulia DiFulvio is an undergraduate research assistant.\nPeter Huang is an undergraduate research assistant."
  },
  {
    "objectID": "data-management-plan.html",
    "href": "data-management-plan.html",
    "title": "Data Management Plan",
    "section": "",
    "text": "The following was updated on 2023-03-01. It is based on document OMB No. 0925-0001 and 0925-0002 (Rev. 07/2022 Approved Through TBD).\nNote that the following contains some URLs provided by the user. These are prohibited in a document submitted to NIH. Since this document is not part of an NIH study, we provide them here for transparency.\n\nIf any of the proposed research in the application involves the generation of scientific data, this application is subject to the NIH Policy for Data Management and Sharing and requires submission of a Data Management and Sharing Plan. If the proposed research in the application will generate large-scale genomic data, the Genomic Data Sharing Policy also applies and should be addressed in this Plan. Refer to the detailed instructions in the application guide for developing this plan as well as to additional guidance on https://sharing.nih.gov. The Plan is recommended not to exceed two pages. Text in italics should be deleted. There is no “form page” for the Data Management and Sharing Plan. The DMS Plan may be provided in the format shown below.\nPublic reporting burden for this collection of information is estimated to average 2 hours per response, including the time for reviewing instructions, searching existing data sources, gathering, and maintaining the data needed, and completing and reviewing the collection of information. An agency may not conduct or sponsor, and a person is not required to respond to, a collection of information unless it displays a currently valid OMB control number. Send comments regarding this burden estimate or any other aspect of this collection of information, including suggestions for reducing this burden, to: NIH, Project Clearance Branch, 6705 Rockledge Drive, MSC 7974, Bethesda, MD 20892-7974, ATTN: PRA (0925-0001 and 0925-0002). Do not return the completed form to this address.\n\n\n\n\nSummarize the types and estimated amount of scientific data expected to be generated in the project.\n\n\n\nDescribe which scientific data from the project will be preserved and shared and provide the rationale for this decision.\n\n\n\nBriefly list the metadata, other relevant data, and any associated documentation (e.g., study protocols and data collection instruments) that will be made accessible to facilitate interpretation of the scientific data.\nThe full protocol, including survey questions is shared publicly via a website on the code/documentation sharing website, GitHub.\n\n\n\n\nState whether specialized tools, software, and/or code are needed to access or manipulate shared scientific data, and if so, provide the name(s) of the needed tool(s) and software and specify how they can be accessed.\n\n\n\nState what common data standards will be applied to the scientific data and associated metadata to enable interoperability of datasets and resources, and provide the name(s) of the data standards that will be applied and describe how these data standards will be applied to the scientific data generated by the research proposed in this project. If applicable, indicate that no consensus standards exist.\nThere are no consensus standards that apply to these data to our knowledge. However, our publicly shared data cleaning code documents that ways that we modified the original dataset to make it more human-readable. CSV text files are considered to be interoperable.\n\n\n\n\n\nProvide the name of the repository(ies) where scientific data and metadata arising from the project will be archived; see Selecting a Data Repository.\nWhen the project is complete and we receive permission to do so, the data may be archived on Databrary (https://databrary.org). Metadata and summary data/visualizations will be archived on GitHub, https://gilmore-lab.github.io/visual-acuity/.\n\n\n\nDescribe how the scientific data will be findable and identifiable, i.e., via a persistent unique identifier or other standard indexing tools.\nIf the project data are shared publicly, Databrary will create a persistent identifier (e.g., DOI).\n\n\n\nDescribe when the scientific data will be made available to other users (i.e., no later than time of an associated publication or end of the performance period, whichever comes first) and for how long data will be available.\n\n\n\n\n\n\nNIH expects that in drafting Plans, researchers maximize the appropriate sharing of scientific data. Describe and justify any applicable factors or data use limitations affecting subsequent access, distribution, or reuse of scientific data related to informed consent, privacy and confidentiality protections, and any other considerations that may limit the extent of data sharing. See Frequently Asked Questions for examples of justifiable reasons for limiting sharing of data.\n\n\n\nState whether access to the scientific data will be controlled (i.e., made available by a data repository only after approval).\nDatabrary does not approve data shared by researchers. Authorization to upload and share data is governed by a formal Databrary Access Agreement that binds Databrary and New York University with a researcher’s home institution. The DAA gives institutionally authorized researchers the right to share data and materials with Databrary consistent with the policies of their institution, ethics board approvals, and the permission of research participants. Similarly, while researchers can share data on Databrary and control or approve individual access themselves, Databrary does not require it.\nOnce shared, data from this project will be available to anyone.\n\n\n\nIf generating scientific data derived from humans, describe how the privacy, rights, and confidentiality of human research participants will be protected (e.g., through de-identification, Certificates of Confidentiality, and other protective measures).\n\n\n\n\nDescribe how compliance with this Plan will be monitored and managed, frequency of oversight, and by whom at your institution (e.g., titles, roles).\nThe PIs on the project will manage these provisions."
  },
  {
    "objectID": "data-management-plan.html#element-1-data-type",
    "href": "data-management-plan.html#element-1-data-type",
    "title": "Data Management Plan",
    "section": "",
    "text": "Summarize the types and estimated amount of scientific data expected to be generated in the project.\n\n\n\nDescribe which scientific data from the project will be preserved and shared and provide the rationale for this decision.\n\n\n\nBriefly list the metadata, other relevant data, and any associated documentation (e.g., study protocols and data collection instruments) that will be made accessible to facilitate interpretation of the scientific data.\nThe full protocol, including survey questions is shared publicly via a website on the code/documentation sharing website, GitHub."
  },
  {
    "objectID": "data-management-plan.html#element-2-related-tools-software-andor-code",
    "href": "data-management-plan.html#element-2-related-tools-software-andor-code",
    "title": "Data Management Plan",
    "section": "",
    "text": "State whether specialized tools, software, and/or code are needed to access or manipulate shared scientific data, and if so, provide the name(s) of the needed tool(s) and software and specify how they can be accessed."
  },
  {
    "objectID": "data-management-plan.html#element-3-standards",
    "href": "data-management-plan.html#element-3-standards",
    "title": "Data Management Plan",
    "section": "",
    "text": "State what common data standards will be applied to the scientific data and associated metadata to enable interoperability of datasets and resources, and provide the name(s) of the data standards that will be applied and describe how these data standards will be applied to the scientific data generated by the research proposed in this project. If applicable, indicate that no consensus standards exist.\nThere are no consensus standards that apply to these data to our knowledge. However, our publicly shared data cleaning code documents that ways that we modified the original dataset to make it more human-readable. CSV text files are considered to be interoperable."
  },
  {
    "objectID": "data-management-plan.html#element-4-data-preservation-access-and-associated-timelines",
    "href": "data-management-plan.html#element-4-data-preservation-access-and-associated-timelines",
    "title": "Data Management Plan",
    "section": "",
    "text": "Provide the name of the repository(ies) where scientific data and metadata arising from the project will be archived; see Selecting a Data Repository.\nWhen the project is complete and we receive permission to do so, the data may be archived on Databrary (https://databrary.org). Metadata and summary data/visualizations will be archived on GitHub, https://gilmore-lab.github.io/visual-acuity/.\n\n\n\nDescribe how the scientific data will be findable and identifiable, i.e., via a persistent unique identifier or other standard indexing tools.\nIf the project data are shared publicly, Databrary will create a persistent identifier (e.g., DOI).\n\n\n\nDescribe when the scientific data will be made available to other users (i.e., no later than time of an associated publication or end of the performance period, whichever comes first) and for how long data will be available."
  },
  {
    "objectID": "data-management-plan.html#element-5-access-distribution-or-reuse-considerations",
    "href": "data-management-plan.html#element-5-access-distribution-or-reuse-considerations",
    "title": "Data Management Plan",
    "section": "",
    "text": "NIH expects that in drafting Plans, researchers maximize the appropriate sharing of scientific data. Describe and justify any applicable factors or data use limitations affecting subsequent access, distribution, or reuse of scientific data related to informed consent, privacy and confidentiality protections, and any other considerations that may limit the extent of data sharing. See Frequently Asked Questions for examples of justifiable reasons for limiting sharing of data.\n\n\n\nState whether access to the scientific data will be controlled (i.e., made available by a data repository only after approval).\nDatabrary does not approve data shared by researchers. Authorization to upload and share data is governed by a formal Databrary Access Agreement that binds Databrary and New York University with a researcher’s home institution. The DAA gives institutionally authorized researchers the right to share data and materials with Databrary consistent with the policies of their institution, ethics board approvals, and the permission of research participants. Similarly, while researchers can share data on Databrary and control or approve individual access themselves, Databrary does not require it.\nOnce shared, data from this project will be available to anyone.\n\n\n\nIf generating scientific data derived from humans, describe how the privacy, rights, and confidentiality of human research participants will be protected (e.g., through de-identification, Certificates of Confidentiality, and other protective measures)."
  },
  {
    "objectID": "data-management-plan.html#element-6-oversight-of-data-management-and-sharing",
    "href": "data-management-plan.html#element-6-oversight-of-data-management-and-sharing",
    "title": "Data Management Plan",
    "section": "",
    "text": "Describe how compliance with this Plan will be monitored and managed, frequency of oversight, and by whom at your institution (e.g., titles, roles).\nThe PIs on the project will manage these provisions."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This page describes what the project is about."
  },
  {
    "objectID": "dashboard.html",
    "href": "dashboard.html",
    "title": "dashboard",
    "section": "",
    "text": "This is a dashboard for the data collection and cleaning process.\nhttps://docs.google.com/spreadsheets/d/1UFZkbh9oU4JHpYsrkDQcNmDyqD4J-qB74dhyMzIkqKs/edit#gid=2144658778 https://docs.google.com/spreadsheets/d/1UFZkbh9oU4JHpYsrkDQcNmDyqD4J-qB74dhyMzIkqKs/edit#gid=0"
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "data",
    "section": "",
    "text": "This page describes the process of data gathering, cleaning, and visualization."
  },
  {
    "objectID": "data.html#overview",
    "href": "data.html#overview",
    "title": "data",
    "section": "",
    "text": "This page describes the process of data gathering, cleaning, and visualization."
  },
  {
    "objectID": "data.html#gathering",
    "href": "data.html#gathering",
    "title": "data",
    "section": "Gathering",
    "text": "Gathering\nWe use a Google Sheet to store the by-study data:\nhttps://docs.google.com/spreadsheets/d/1UFZkbh9oU4JHpYsrkDQcNmDyqD4J-qB74dhyMzIkqKs/edit#gid=0\n\n\n\n\n\n\nNote\n\n\n\nNote: There is no identifiable data here at the moment, so Google Sheets are a viable option.\nLater on, we start contacting authors, we will need to restrict access to that information for privacy reasons.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWe need a process for managing who has edit access.\n\n\nThe googledrive package provides a convenient way to access documents stored on Google.\n\nDownload from Google as CSV\n\nif (!dir.exists(params$data_dir)) {\n  message(\"Creating missing \",  params$data_dir, \".\")\n  dir.create(params$data_dir)\n}\n\nif (params$update_data) {\n  if (params$use_sysenv_creds) {\n    google_creds &lt;- Sys.getenv(\"GMAIL_SURVEY\")\n    if (google_creds != \"\") {\n      options(gargle_oauth_email = google_creds)\n      googledrive::drive_auth()\n    } else {\n      message(\"No Google account information stored in `.Renviron`.\")\n      message(\"Add authorized Google account name to `.Renviron` using `usethist::edit_r_environ()`.\")\n    }\n  }\n\n  googledrive::drive_download(\n    params$google_data_fn,\n    path = file.path(params$data_dir, \"by-paper.csv\"),\n    type = \"csv\",\n    overwrite = TRUE\n  )\n  message(\"Data updated.\")\n} else {\n  message(\"Using stored data.\")\n}\n\nFile downloaded:\n\n\n• 'Legacy Project Acuity Data: By Paper'\n  &lt;id: 1UFZkbh9oU4JHpYsrkDQcNmDyqD4J-qB74dhyMzIkqKs&gt;\n\n\nSaved locally as:\n\n\n• 'data/csv/by-paper.csv'\n\n\nData updated.\n\n\nThe data file has been saved as a comma-separated value (CSV) format data file in a special directory called csv/.\n\n\nOpen CSV\nNext we load the data file.\n\nacuity_df &lt;-\n  readr::read_csv(file.path(params$data_dir, \"by-paper.csv\"), show_col_types = FALSE)\n\nWe’ll show the column (variable names) since these will be part of our data dictionary.\n\nacuity_cols &lt;- names(acuity_df)\nacuity_cols\n\n[1] \"analyst_name\"     \"analyst_initials\"\n\n\n\n\nCreate data dictionary\nWe’ll start by creating a data dictionary so that we can refer to it later in our cleaning and data analysis. We do this by creating a data frame or ‘tibble’ because this is a convenient format for manipulating the information.\n\nacuity_data_dict &lt;- tibble::tibble(col_name = names(acuity_df))\n\nNow, we write a short description of each variable in the data file.\n\nacuity_data_dict &lt;- acuity_data_dict |&gt;\n  dplyr::mutate(col_desc = c(\"Last name of 1st author\",\n                             \"Full APA format citation\",\n                             \"Paper publication year\",\n                             \"Source in paper\",\n                             \"Reported age range in mos\",\n                             \"Age in mos as conformed by ROG\",\n                             \"Participants tested monocularly or binocularly\",\n                             \"Number of participants in group\",\n                             \"Testing distance in cm\",\n                             \"Starting card in cyc/deg\",\n                             \"Mean (group) acuity in cyc/deg\",\n                             \"Estimated lower limit of acuity in cyc/deg\",\n                             \"Teller Acuity Card closest equivalent to this lower limit\",\n                             \"Estimated upper limit of acuity in cyc/deg\",\n                             \"Country where data were collected\",\n                             \"TAC-I or TAC-II\"))\n\nacuity_data_dict |&gt;\n  knitr::kable(format = 'html') |&gt;\n  kableExtra::kable_classic()"
  },
  {
    "objectID": "data.html#data-visualization",
    "href": "data.html#data-visualization",
    "title": "data",
    "section": "Data visualization",
    "text": "Data visualization\n\n\n\n\n\n\nImportant\n\n\n\nRick Gilmore decided to take the mean of the age range reported in the (Xiang et al., 2021) data and create a new variable strictly for visualization purposes, age_grp_rog.\n\n\nWe are still in the early phases of the project (as of 2024-03-19 08:36:53.912568), but it is good to start sketching the the data visualizations we will eventually want to see.\n\nlibrary(ggplot2)\nacuity_df |&gt;\n  ggplot() +\n  aes(x = age_grp_rog, y = mean_acuity_cyc_deg, color = country) +\n  geom_point() +\n  geom_smooth() +\n  facet_grid(cols = vars(binoc_monoc))"
  },
  {
    "objectID": "data.html#by-individual-data",
    "href": "data.html#by-individual-data",
    "title": "data",
    "section": "By-individual data",
    "text": "By-individual data\nThe Gilmore lab has some archival data that we can potentially use in this project. The following represents Rick Gilmore’s work to gather, clean, and visualize these data.\n\nGathering\nThe de-identified archival data are stored in a Google sheet accessed by the lab Google account.\nFirst, we must authenticate to Google to access the relevant file and download it.\n\noptions(gargle_oauth_email = \"psubrainlab@gmail.com\")\ngoogledrive::drive_auth()\n\nThen we download the relevant file.\n\ngoogledrive::drive_download(\n  \"vep-session-log\",\n  path = file.path(params$data_dir, \"by-participant.csv\"),\n  type = \"csv\",\n  overwrite = TRUE\n)\n\nUnlike the Google sheet newly created for the by-study data, this one requires a lot of cleaning.\n\ngilmore_archival_df &lt;-\n  readr::read_csv(file.path(params$data_dir, \"by-participant.csv\"),\n                  show_col_types = FALSE)\nnames(gilmore_archival_df)\n\nWe’ll keep Date, Time, Sex, DOB, Teller Acuity Cards, Age at test.\n\ngilmore_archival_df &lt;- gilmore_archival_df |&gt;\n  dplyr::select(Date, Time, Sex, DOB, `Teller Acuity Cards`, `Age at test`)\n\nThen, let’s filter those where we have TAC data.\n\nwith(gilmore_archival_df, unique(`Teller Acuity Cards`))\n\n\ngilmore_archival_df &lt;- gilmore_archival_df |&gt;\n  dplyr::filter(!is.na(`Teller Acuity Cards`),\n                `Teller Acuity Cards` != \"not interested\")\n\ndim(gilmore_archival_df)\n\n\n\n\n\n\n\nNote\n\n\n\nThis file illustrates how making data FAIR from the outset can save work.\nThis one is not too terribly hard to parse, but it could have been better planned.\n\n\nWe’ll extract the viewing distance with a regular expression.\n\ngilmore_archival_df &lt;- gilmore_archival_df |&gt;\n  dplyr::mutate(view_dist_cm = stringr::str_extract(`Teller Acuity Cards`, \"[0-9]{2}cm\")) |&gt;\n  dplyr::mutate(view_dist_cm = stringr::str_remove(view_dist_cm, \"cm\")) # remove 'cm'\ngilmore_archival_df$view_dist_cm\n\nSimilarly, we’ll extract the acuity in cyc/deg using a regular expression.\n\ngilmore_archival_df &lt;- gilmore_archival_df |&gt;\n  # add 'cyc' to separate cyc/deg from Snellen acuity\n  dplyr::mutate(acuity_cyc_deg = stringr::str_extract(`Teller Acuity Cards`, \"[0-9]{1}[\\\\./]{1}[0-9]+ cyc\")) |&gt;\n  dplyr::mutate(acuity_cyc_deg = stringr::str_remove(acuity_cyc_deg, \" cyc\")) |&gt;\n  dplyr::mutate(acuity_cyc_deg = stringr::str_replace(acuity_cyc_deg, \"/\", \".\"))\n\ngilmore_archival_df$acuity_cyc_deg\n\nNow, let’s look at the age at test.\n\ngilmore_archival_df$`Age at test`\n\nInstead, let’s see what it looks like to compute age at test from the dates.\n\ngilmore_archival_df &lt;- gilmore_archival_df |&gt;\n  dplyr::mutate(age_at_test_days = lubridate::mdy(Date) - lubridate::mdy(DOB))\n\ngilmore_archival_df$age_at_test_days\n\nThat seems reasonable for now.\nLet’s see if we can plot these data.\n\ngilmore_archival_df |&gt;\n  dplyr::mutate(acuity_cyc_deg = as.numeric(acuity_cyc_deg)) |&gt;\n  ggplot() +\n  aes(x = age_at_test_days, y = acuity_cyc_deg, color = Sex) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  #theme_classic() +\n  theme(legend.position = \"bottom\", legend.title = element_blank()) \n\n\n\n\n\n\n\nNote\n\n\n\nBefore I stop, I’m going to add the by-participant data file to a .gitignore file, just to be extra careful."
  },
  {
    "objectID": "paper-summary.html#summarize-captured-paper-data",
    "href": "paper-summary.html#summarize-captured-paper-data",
    "title": "Paper summary",
    "section": "Summarize captured paper data",
    "text": "Summarize captured paper data"
  },
  {
    "objectID": "dashboard.html#download",
    "href": "dashboard.html#download",
    "title": "dashboard",
    "section": "Download",
    "text": "Download\n\nif (!dir.exists(params$data_dir)) {\n  message(\"Creating missing \",  params$data_dir, \".\")\n  dir.create(params$data_dir)\n}\n\nif (params$update_data) {\n  if (params$use_sysenv_creds) {\n    google_creds &lt;- Sys.getenv(\"GMAIL_SURVEY\")\n    if (google_creds != \"\") {\n      options(gargle_oauth_email = google_creds)\n      googledrive::drive_auth()\n    } else {\n      message(\"No Google account information stored in `.Renviron`.\")\n      message(\"Add authorized Google account name to `.Renviron` using `usethist::edit_r_environ()`.\")\n    }\n  }\n\n  googledrive::drive_download(\n    params$google_data_fn,\n    path = file.path(params$data_dir, \"by-paper.csv\"),\n    type = \"csv\",\n    overwrite = TRUE\n  )\n  message(\"Data updated.\")\n} else {\n  message(\"Using stored data.\")\n}\n\nFile downloaded:\n\n\n• 'Legacy Project Acuity Data: By Paper'\n  &lt;id: 1UFZkbh9oU4JHpYsrkDQcNmDyqD4J-qB74dhyMzIkqKs&gt;\n\n\nSaved locally as:\n\n\n• 'data/csv/by-paper.csv'\n\n\nData updated.\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe have a separate tracking sheet that needs to be integrated into this workflow."
  },
  {
    "objectID": "dashboard.html#clean",
    "href": "dashboard.html#clean",
    "title": "dashboard",
    "section": "Clean",
    "text": "Clean"
  },
  {
    "objectID": "dashboard.html#visualize",
    "href": "dashboard.html#visualize",
    "title": "dashboard",
    "section": "Visualize",
    "text": "Visualize\n\nTime series of data collections by time.\nNumber of papers\nBy dates\nWho entered\nOpened successfully\n\n\nOn PSU network\nAccess via PSU Libraries"
  }
]