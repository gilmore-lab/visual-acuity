---
title: "Dashboard"
format:
  html:
    code-fold: true
    code-tools: true
    toc: true
params:
  data_dir: "data/csv"
  update_data: TRUE
  use_sysenv_creds: TRUE
  google_data_url: "https://docs.google.com/spreadsheets/d/1UFZkbh9oU4JHpYsrkDQcNmDyqD4J-qB74dhyMzIkqKs/edit?usp=sharing"
  sheet_name: "paper_data"
  data_fn: "paper-sources.csv"
---

This is a dashboard for the data collection and cleaning process.

All code is "folded" by default.
Select "Show All Code" from the menu at the upper right to reveal the code chunks.

## Set-up

We load `ggplot2` to make the following plot commands easier to type.

```{r}
library(ggplot2)
```

## Download

The data are stored in a Google sheet that we download if `params$update_data == TRUE`.

```{r}
if (!dir.exists(params$data_dir)) {
  message("Creating missing ",  params$data_dir, ".")
  dir.create(params$data_dir)
}

project_ss <- params$google_data_url

if (params$update_data) {
  if (params$use_sysenv_creds) {
    google_creds <- Sys.getenv("GMAIL_SURVEY")
    if (google_creds != "") {
      options(gargle_oauth_email = google_creds)
      googledrive::drive_auth()
    } else {
      message("No Google account information stored in `.Renviron`.")
      message("Add authorized Google account name to `.Renviron` using `usethist::edit_r_environ()`.")
    }
  }

  papers_data <- googlesheets4::read_sheet(ss = project_ss,
                            sheet = params$sheet_name)
  out_fn <- file.path(params$data_dir, params$data_fn)
  readr::write_csv(papers_data, out_fn)
  message("Data updated: ", out_fn)
} else {
  message("Using stored data.")
  papers_data <- readr::read_csv(file.path(params$data_dir, params$data_fn),
                                 show_col_types = FALSE)
}
```

### Synched Paperpile file from GitHub

We have configured Paperpile to synch a .bib formatted file directly with this repo on GitHub.
The files can be found here: `src/data/*paperpile*.bib`.

We import `data/paperpile-tac-has-pdf.bib` and `data/paperpile-tac-no-pdf.bib` separately; add a variable indicating whether we have or do not have a PDF; then, join the two data data frames.

```{r}
refs_w_pdf <- bib2df::bib2df("data/paperpile-tac-has-pdf.bib", separate_names = TRUE)
refs_no_pdf <- bib2df::bib2df("data/paperpile-tac-no-pdf.bib", separate_names = TRUE)

refs_w_pdf <- refs_w_pdf |>
  dplyr::mutate(pdf = TRUE)

refs_no_pdf <- refs_no_pdf |>
  dplyr::mutate(pdf = FALSE)

refs <- dplyr::full_join(refs_w_pdf, refs_no_pdf)
```

## Clean

The author and editor fields are imported as lists.
We need to merge these into character strings to re-import the data back into Google Sheets.

```{r}
# Create function to change AUTHOR list to a string array
make_author_list <- function(df) {
  unlist(df$full_name) |> paste(collapse = "; ")
}

make_editor_list <- function(df) {
  if (is.na(df$full_name)) {
    ""
  } else {
  unlist(df$full_name) |> paste(collapse = "; ")    
  }
}

authors_string <- purrr::map(refs$AUTHOR, make_author_list) |> 
  purrr::list_c()

editors_string <- purrr::map(refs$EDITOR, make_author_list) |> 
  purrr::list_c()

new_refs <- refs |>
  dplyr::mutate(authors = authors_string,
                editors = editors_string) |>
  dplyr::select(-c("AUTHOR", "ANNOTE", "EDITOR"))
```

## Upload cleaned

We the push the cleaned data back to Google Sheets for further analysis and processing.

::: {.callout-warning}

We do not push the cleaned data back to the original sheet but to a new one to avoid overwriting data.

:::

```{r}
new_refs |> 
  googlesheets4::sheet_write(project_ss, "from_paperpile_via_github_cleaned")
```

## Visualize

### Papers by publication date

The following uses the new has-pdf/no-pdf export workflow from Paperpile directly to Github.

```{r}
#| label: fig-papers-by-pub-year-paperpile
#| fig-cap: "Papers by publication year"
refs |>
  ggplot() +
  aes(x = YEAR, fill = pdf) +
  geom_bar() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```

## Extracted tables

This section extracts data about our progress in capturing data tables from these articles.

```{r}
img_folder <- googledrive::drive_find(type = "folder", q = "name contains 'legacy'")
img_df <- googledrive::drive_ls(img_folder)

img_df <- img_df |>
  dplyr::mutate(paper_id = stringr::str_extract_all(name, "[a-zA-Z0-9]+\\-[a-z]{2}"))
```

Now that we have re-extracted the `paper_id`, we can do some summaries.

```{r}
n_tables <- dim(img_df)[1]
n_papers <- length(unique(img_df$paper_id))
```

We've processed `{r} n_papers` papers and `{r} n_tables` tables as of `{r} Sys.Date()`.

<!-- ### Papers entered by analyst -->

<!-- ```{r} -->
<!-- #| label: fig-papers-by-analyst -->
<!-- #| fig-cap: "Papers entered by analyst" -->
<!-- papers_data |> -->
<!--   ggplot() + -->
<!--   aes(x = entered_by) + -->
<!--   geom_bar() -->
<!-- ``` -->


<!-- ### Papers by open status -->

<!-- #### On PSU network -->

<!-- ```{r} -->
<!-- #| label: fig-openable-papers -->
<!-- #| fig-cap: "Papers with openable URLs" -->
<!-- papers_data |> -->
<!--   ggplot() + -->
<!--   aes(x = url_openable) + -->
<!--   geom_bar() -->
<!-- ``` -->

<!-- #### Access via PSU Libraries -->
